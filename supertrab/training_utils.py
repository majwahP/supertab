"""
Training and evaluation utilities for DDPM-based 2D super-resolution on HR-pQCT trabecular bone data.

This module defines:
- A dataclass for configuring training parameters.
- Utilities to visualize super-resolution results as image grids.
- The training loop for a 2D DDPM with classifier-free guidance.
- An evaluation function to generate SR images and log outputs.

Training is handled using Hugging Face's `diffusers` library and accelerated using `accelerate`. 
Low-resolution images are used as conditioning inputs to the denoising U-Net.

Code is based on: https://huggingface.co/docs/diffusers/en/tutorials/basic_training
"""


import torch.nn.functional as F
import torch
import torchvision.utils as vutils
from PIL import Image 
from torchvision.transforms.functional import to_pil_image
import os
from accelerate import Accelerator
from PIL import Image, ImageDraw, ImageFont
import PIL
import wandb
import shutil
from supertrab.metrics_utils import compute_image_metrics
from diffusers import UNet2DModel, DDPMScheduler



def normalize_tensor(tensor):
    """
    Normalizes a PyTorch tensor to the [0, 1] range.

    Args:
        tensor (torch.Tensor): Input tensor.

    Returns:
        torch.Tensor: Normalized tensor.
    """
    return (tensor - tensor.min()) / (tensor.max() - tensor.min() + 1e-8)

def create_sample_image(lr_imgs, sr_imgs, hr_imgs, padding=10, header_height=60, footer_height=40, metrics=None):
    """
    Generates a visual grid showing low-resolution (LR), super-resolved (SR), high-resolution (HR), and difference (|SR - HR|) images.

    Args:
        lr_imgs (List[Tensor]): List of LR images.
        sr_imgs (List[Tensor]): List of SR images generated by the model.
        hr_imgs (List[Tensor]): List of corresponding ground truth HR images.
        padding (int): Padding between image columns and rows.
        header_height (int): Height reserved for column labels.
        footer_height (int): Height reserved for MSE text below each row.

    Returns:
        PIL.Image: Composited grid image for visual comparison.
    """

    assert len(lr_imgs) == len(sr_imgs) == len(hr_imgs), "Image lists must be same length"
    n = len(lr_imgs)

    # Convert to PIL
    lr_pil = [to_pil_image(normalize_tensor(img.squeeze(0))) for img in lr_imgs]
    sr_pil = [to_pil_image(normalize_tensor(img.squeeze(0))) for img in sr_imgs]
    hr_pil = [to_pil_image(normalize_tensor(img.squeeze(0))) for img in hr_imgs]
    diff_pil = [to_pil_image(normalize_tensor(torch.abs(sr - hr).squeeze(0))) for sr, hr in zip(sr_imgs, hr_imgs)]
    

    # Assume all images have the same size
    w, h = lr_pil[0].size

    font_size = max(10, h // 10)
    font_path = PIL.__path__[0] + "/fonts/DejaVuSans.ttf"
    font = ImageFont.truetype(font_path, font_size)

    # Full grid size
    total_width = 4 * w + 5 * padding
    total_height = n * (h + footer_height + padding) + header_height + padding

    grid_img = Image.new("L", (total_width, total_height), color=255)
    draw = ImageDraw.Draw(grid_img)

    # Draw headers
    headers = ["LR", "SR", "HR", "Diff"]
    for i, text in enumerate(headers):
        x = padding + i * (w + padding) + w // 2
        draw.text((x, header_height // 2), text, fill=0, anchor="mm", font=font)

    # Draw each sample row
    for idx in range(n):
        y_offset = header_height + padding + idx * (h + footer_height + padding)

        x_offsets = [padding + i * (w + padding) for i in range(4)]
        images = [lr_pil[idx], sr_pil[idx], hr_pil[idx], diff_pil[idx]]

        for x, img in zip(x_offsets, images):
            grid_img.paste(img, (x, y_offset))

        # Draw metrics below diff image
        if metrics:
            mse = metrics[idx]["mse"]
            psnr = metrics[idx]["psnr"]
            ssim = metrics[idx]["ssim"]
            lpips_val = metrics[idx]["lpips"]

            metric_x = x_offsets[3] + w // 2
            metric_y = y_offset + h + footer_height // 4

            metric_text_1 = f"MSE: {mse:.4f}  |  PSNR: {psnr:.2f} dB"
            draw.text((metric_x, metric_y), metric_text_1, fill=0, anchor="mm", font=font)

            line_spacing = font.size + 2  # adjust spacing as needed
            metric_text_2 = f"SSIM: {ssim:.4f}  |  LPIPS: {lpips_val:.4f}"
            draw.text((metric_x, metric_y + line_spacing), metric_text_2, fill=0, anchor="mm", font=font)


    return grid_img


def save_sr_outputs(lr_imgs, sr_imgs, hr_imgs, save_dir, base_name):
    """
    Save LR, SR, and HR images (as .png) to separate files.

    Args:
        lr_imgs (List[Tensor]): List of low-resolution images.
        sr_imgs (List[Tensor]): List of super-resolved images.
        hr_imgs (List[Tensor]): List of high-resolution images.
        save_dir (Path or str): Directory to save the images.
        base_name (str): Base name for the saved files (e.g., "epoch_001").
    """
    os.makedirs(save_dir, exist_ok=True)

    for idx, (lr, sr, hr) in enumerate(zip(lr_imgs, sr_imgs, hr_imgs)):
        idx_str = f"{idx:03d}"
        vutils.save_image(normalize_tensor(lr), os.path.join(save_dir, f"{base_name}_lr_{idx_str}.png"))
        vutils.save_image(normalize_tensor(sr), os.path.join(save_dir, f"{base_name}_sr_{idx_str}.png"))
        vutils.save_image(normalize_tensor(hr), os.path.join(save_dir, f"{base_name}_hr_{idx_str}.png"))

def generate_sr_images(model, scheduler, lr_images, target_size, device):
    """
    Generates super-resolved images using the reverse DDPM process.
    """
    lr_resized = F.interpolate(lr_images, size=(target_size, target_size), mode="bilinear", align_corners=False)
    noisy_images = torch.randn((lr_images.size(0), 1, target_size, target_size), device=device)

    for t in reversed(range(scheduler.config.num_train_timesteps)):
        timesteps = torch.full((lr_images.size(0),), t, device=device, dtype=torch.long)
        model_input = torch.cat([noisy_images, lr_resized], dim=1)
        noise_pred = model(model_input, timesteps).sample
        noisy_images = scheduler.step(noise_pred, t, noisy_images).prev_sample

    return noisy_images

def log_metrics_and_artifacts(image_to_save, batch_metrics, epoch, global_step, output_dir):
    """
    Logs evaluation results to WandB and saves them as an artifact.
    """
    avg_mse = sum(m["mse"] for m in batch_metrics) / len(batch_metrics)
    avg_psnr = sum(m["psnr"] for m in batch_metrics) / len(batch_metrics)

    if wandb.run is not None:
        log_dict = {
            "eval/mse": avg_mse,
            "eval/psnr": avg_psnr,
        }

        if image_to_save is not None:
            log_dict[f"sample_epoch_{epoch}"] = wandb.Image(image_to_save)

        wandb.log(log_dict, step=global_step)

        if output_dir is not None and os.path.exists(output_dir):
            artifact_name = f"eval_epoch_{epoch}"
            zip_path = shutil.make_archive(output_dir, 'zip', output_dir)

            artifact = wandb.Artifact(name=artifact_name, type="evaluation_outputs")
            artifact.add_file(zip_path)
            wandb.run.log_artifact(artifact)

def save_checkpoint(model, optimizer, epoch, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }, path)


@torch.no_grad()
def evaluate(config, epoch, model, noise_scheduler, dataloader, device="cuda", global_step=None):
    """
    Generates super-resolved images from the model and saves a grid comparing LR, SR, and HR images.

    Args:
        config (TrainingConfig): Configuration dataclass with experiment settings.
        epoch (int or str): Current epoch number (used for naming output).
        model (UNet2DModel): Trained denoising model.
        noise_scheduler (DDPMScheduler): Scheduler for reverse diffusion steps.
        dataloader (DataLoader): DataLoader for evaluation patches.
        device (str): Device to run evaluation on.
        global_step (int or None): Optional global step for logging to Weights & Biases.
    """
    model.eval()
    model.to(device)

    batch = next(iter(dataloader))
    lr_images, hr_images = batch["lr_image"].to(device), batch["hr_image"].to(device)

    sr_images = generate_sr_images(model, noise_scheduler, lr_images, config.image_size, device)

    # Clamp and move to CPU for visualization
    sr_images = sr_images.clamp(0.0, 1.0).cpu()
    lr_images_up = F.interpolate(lr_images, size=(config.image_size, config.image_size),
                                 mode="bilinear", align_corners=False).clamp(0.0, 1.0).cpu()
    hr_images = hr_images.cpu().clamp(0.0, 1.0)

    # Compute metrics and create image grid
    batch_metrics = compute_image_metrics(sr_images, hr_images)

    # Only save images every 10th epoch and on last epoch
    save_images = (
        isinstance(epoch, int)
        and (epoch == 0 or epoch % config.save_image_epochs == 0 or epoch == config.num_epochs - 1)
    )

    if save_images:
        base_name = f"{epoch:04d}_ds{config.ds_factor}_size{config.image_size}.png"
        output_subdir = os.path.join(config.output_dir, f"{config.image_size}_ds{config.ds_factor}/images", base_name) # Name change
        os.makedirs(output_subdir, exist_ok=True)

        image_to_save = create_sample_image(lr_images_up, sr_images, hr_images, metrics=batch_metrics)
        image_to_save.save(os.path.join(output_subdir, "overview.png"))
        save_sr_outputs(lr_images_up, sr_images, hr_images, output_subdir, base_name)
    else:
        output_subdir = None  # avoid invalid path use in logging

    # Log metrics and artifacts
    log_metrics_and_artifacts(image_to_save if save_images else None, batch_metrics, epoch, global_step, output_subdir)


def train_loop_2D_diffusion(config, model, noise_scheduler, optimizer, train_dataloader, val_dataloader, lr_scheduler, steps_per_epoch, starting_epoch = 0):
    """
    Trains a UNet-based 2D DDPM model for super-resolution with classifier-free guidance.

    Args:
        config (TrainingConfig): Training configuration.
        model (UNet2DModel): Diffusion model to train.
        noise_scheduler (DDPMScheduler): Scheduler for forward diffusion steps.
        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.
        train_dataloader (DataLoader): DataLoader for training patches.
        val_dataloader (DataLoader): DataLoader for validation patches.
        lr_scheduler (Scheduler): Learning rate scheduler.
        steps_per_epoch (int): Number of training steps per epoch.
        starting_epoch (int): from which epoch we are training, allows resuming training
    """

    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="wandb",
        project_dir=os.path.join(config.output_dir, "logs"),
    )
    # assure only one process creates directories or logs
    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        run_name = f"supertrab_ddpm_{config.image_size}px_ds{config.ds_factor}_{config.num_epochs}ep_v5" # Name change
        accelerator.init_trackers(
            project_name="supertrab", 
            config=vars(config),
            init_kwargs={"wandb": {"name": run_name}}
        )


    # Prepare everything
    # There is no specific order to remember, you just need to unpack the
    # objects in the same order you gave them to the prepare method.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # Now you train the model
    for epoch in range(starting_epoch, config.num_epochs):
        if accelerator.is_local_main_process:
            print(f"Starting Epoch {epoch}...")


        for step, batch in enumerate(train_dataloader):
            if step >= steps_per_epoch:
                break
            clean_images = batch["hr_image"]
            conditioning = batch["lr_image"]
            # Sample noise to add to the images
            noise = torch.randn_like(clean_images)
            

            # Sample a random timestep for each image
            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (clean_images.size(0),), device=clean_images.device)

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)


            # Backpropagation
            with accelerator.accumulate(model):
                # Predict the noise residual
                conditioning_resized = F.interpolate(conditioning, size=clean_images.shape[-2:], mode='bilinear', align_corners=False)

                # Classifier-Free Guidance: randomly drop conditioning
                if torch.rand(1).item() < config.cfg_dropout_prob:
                    conditioning_resized = torch.zeros_like(conditioning_resized)


                # Concatenate LR with noisy HR as input to the model
                model_input = torch.cat([noisy_images, conditioning_resized], dim=1)

                # Predict noise
                noise_pred = model(model_input, timesteps).sample
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                # Avoid exploding gradients
                if accelerator.sync_gradients:
                    accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            accelerator.log(logs, step=global_step)
            global_step += 1

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if accelerator.is_main_process:
            #evaluate after every epoch
            evaluate(config, epoch, model, noise_scheduler, val_dataloader, device=accelerator.device, global_step=global_step)

            # Save final model and inference weights at last epoch
            if epoch == config.num_epochs - 1:
                models_dir = os.path.join(config.output_dir, f"{config.image_size}_ds{config.ds_factor}/models") # Name change
                os.makedirs(models_dir, exist_ok=True)

                final_ckpt_path = os.path.join(models_dir, f"final_training_checkpoint_{config.image_size}_ds{config.ds_factor}.pth") # Name change
                weights_path = os.path.join(models_dir, f"final_model_weights_{config.image_size}_ds{config.ds_factor}.pth") # Name change

                # Save full training checkpoint
                save_checkpoint(accelerator.unwrap_model(model), optimizer, epoch+1, final_ckpt_path)
                # artifact = wandb.Artifact("final_training_checkpoint", type="model")
                # artifact.add_file(final_ckpt_path)
                # wandb.log_artifact(artifact)

                # Save inference weights only
                torch.save(accelerator.unwrap_model(model).state_dict(), weights_path)
                # artifact = wandb.Artifact("final_model_weights", type="model")
                # artifact.add_file(weights_path)
                # wandb.log_artifact(artifact)


def load_model_and_optimizer(config, checkpoint_path):
    model = UNet2DModel(
        sample_size=config.image_size,
        in_channels=2,
        out_channels=1,
        layers_per_block=2,
        block_out_channels=(128, 128, 256, 256, 512, 512),
        down_block_types=("DownBlock2D", "DownBlock2D", "DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D"),
        up_block_types=("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D", "UpBlock2D"),
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)

    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model_state_dict"])
    optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    start_epoch = checkpoint["epoch"]

    return model, optimizer, start_epoch
